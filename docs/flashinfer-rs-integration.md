# flashinfer-rs Integration Notes

## What a wheel is here
A Python wheel (`*.whl`) is a zip archive that carries prebuilt artifacts. For this integration we treat wheels as an artifact container and extract only the shared libraries we need:

- `flashinfer_jit_cache/.../jit_cache/norm/norm.so`
- `flashinfer_jit_cache/.../jit_cache/gdn_prefill_sm90/gdn_prefill_sm90.so`
- `tvm_ffi/lib/libtvm_ffi.so`

No Python runtime is required for calling `gemma_rmsnorm` once the `.so` files are extracted.

## Artifact Download URLs
`libtvm_ffi.so` source:

- Apache TVM-FFI package page: `https://pypi.org/project/apache-tvm-ffi/`
- Apache TVM-FFI wheel index (direct wheel links): `https://pypi.org/simple/apache-tvm-ffi/`

`libtvm_ffi.so` is extracted from the wheel path:

- `tvm_ffi/lib/libtvm_ffi.so`

Pre-built FlashInfer wheel sources:

- Stable CUDA 13.0 index root: `https://flashinfer.ai/whl/cu130/`
- Stable `flashinfer-jit-cache` wheel list: `https://flashinfer.ai/whl/cu130/flashinfer-jit-cache/`
- Nightly CUDA 13.0 index root: `https://flashinfer.ai/whl/nightly/cu130/`
- Nightly `flashinfer-jit-cache` wheel list: `https://flashinfer.ai/whl/nightly/cu130/flashinfer-jit-cache/`
- Installation guide: `https://docs.flashinfer.ai/installation.html`

## Why this v1 uses wheels instead of `flashinfer-cubin`
`gemma_rmsnorm` is already exposed from the host wrapper symbol `__tvm_ffi_gemma_rmsnorm` in `norm.so`. That wrapper is the stable user-facing entry for kernel launch and argument checking.

`flashinfer-cubin` is useful for workflows that separately download cubins and wire callback loaders. This v1 does not need that because `norm.so` already ships the compiled device code used by the wrapper.

## Cubin symbols vs API symbols
Cubin-level symbols are low-level, mangled kernel entry names generated by CUDA/C++. They are not intended as a stable external API.

The Rust integration calls the exported TVM-FFI host wrapper:

- `__tvm_ffi_gemma_rmsnorm`
- `__tvm_ffi_gdn_prefill`

This wrapper handles argument decoding, validation, stream lookup, and dispatch to the correct kernel implementation.

## Dependency/artifact matrix
Pinned v1 artifacts:

- `flashinfer_jit_cache 0.6.3+cu130`
- `apache_tvm_ffi 0.1.3`

Runtime loading order:

1. Load `libtvm_ffi.so` with `RTLD_NOW | RTLD_GLOBAL`
2. Load `norm.so` with `RTLD_NOW | RTLD_LOCAL`
3. Load `gdn_prefill_sm90.so` with `RTLD_NOW | RTLD_LOCAL`

Required CUDA runtime dependency from `norm.so`:

- `libcudart.so.13`

`gdn_prefill_sm90` dispatch is `sm_90a`-only in the shipped kernel launcher. On non-SM90 GPUs the call will fail with a decoded TVM-FFI error.

## Architecture-Dependent Kernel Handling
FlashInfer ships architecture-dependent device code and host wrappers that dispatch at runtime.

Packaging model:

1. `flashinfer-cubin`: pre-compiled kernel binaries covering supported architectures.
2. `flashinfer-jit-cache`: pre-built JIT cache bundles for specific CUDA versions.
3. `flashinfer-python`: Python package that can JIT/download kernels on first use.

Runtime behavior in this Rust integration:

1. Rust calls exported host wrappers such as `__tvm_ffi_gemma_rmsnorm` and `__tvm_ffi_gdn_prefill`.
2. The wrapper checks tensor contracts and device properties, then selects an architecture-specific kernel path.
3. If the loaded artifact does not include a compatible kernel for the active GPU, the wrapper raises an error (decoded as `FlashInferError::TvmFfiCall`).

Concrete example:

- `gdn_prefill` checks device major capability and only dispatches the SM90A implementation in the current launcher (`flashinfer/csrc/gdn_prefill_launcher.cu`).

Build-from-source note:

- FlashInfer uses `FLASHINFER_CUDA_ARCH_LIST` to control which architectures are compiled into generated artifacts.

## Runtime configuration knobs
Environment variables accepted by `flashinfer-rs`:

- `FLASHINFER_RS_JIT_CACHE_WHEEL`
- `FLASHINFER_RS_TVMFFI_WHEEL`
- `FLASHINFER_RS_CACHE_DIR`

Extracted artifacts are cached at:

- `~/.cache/flashinfer-rs/<artifact-hash>/`

with a lock file to avoid concurrent extraction races.
