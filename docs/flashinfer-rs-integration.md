# flashinfer-rs Integration Notes

## What a wheel is here
A Python wheel (`*.whl`) is a zip archive that carries prebuilt artifacts. For this integration we treat wheels as an artifact container and extract only the shared libraries we need:

- `flashinfer_jit_cache/.../jit_cache/norm/norm.so`
- `flashinfer_jit_cache/.../jit_cache/gdn_prefill_sm90/gdn_prefill_sm90.so`
- `flashinfer_jit_cache/.../jit_cache/page/page.so`
- `flashinfer_jit_cache/.../jit_cache/tgv_gemm_fp16/tgv_gemm_fp16.so`
- `flashinfer_jit_cache/.../jit_cache/tgv_gemm_bf16/tgv_gemm_bf16.so`
- `flashinfer_jit_cache/.../jit_cache/trtllm_gemm/trtllm_gemm.so`
- `flashinfer_jit_cache/.../jit_cache/trtllm_low_latency_gemm/trtllm_low_latency_gemm.so`
- `flashinfer_jit_cache/.../jit_cache/single_prefill_with_kv_cache_.../single_prefill_with_kv_cache_....so`
- `flashinfer_jit_cache/.../jit_cache/batch_prefill_with_kv_cache_.../batch_prefill_with_kv_cache_....so`
- `flashinfer_jit_cache/.../jit_cache/single_decode_with_kv_cache_.../single_decode_with_kv_cache_....so`
- `flashinfer_jit_cache/.../jit_cache/batch_decode_with_kv_cache_.../batch_decode_with_kv_cache_....so`
- `tvm_ffi/lib/libtvm_ffi.so`

No Python runtime is required for calling `gemma_rmsnorm` once the `.so` files are extracted.

Build/runtime model in this crate:

1. `build.rs` selects pinned wheel entries from `Cargo.toml` by build-host CUDA version (`cu130`/`cu131`) and target architecture (`x86_64` or `aarch64`).
2. `build.rs` emits selected wheel metadata (filename, URL, SHA256) as generated constants.
3. Runtime downloads selected wheels into `~/.cache/flashinfer-rs/wheels/` (or `FLASHINFER_RS_CACHE_DIR/wheels/`) on cache miss.
4. Runtime verifies cached wheel SHA256 and rewrites mismatches.
5. Runtime extracts required `.so` members from cached wheel files.

## Artifact Download URLs
`libtvm_ffi.so` source:

- Apache TVM-FFI package page: `https://pypi.org/project/apache-tvm-ffi/`
- Apache TVM-FFI wheel index (direct wheel links): `https://pypi.org/simple/apache-tvm-ffi/`

`libtvm_ffi.so` is extracted from the wheel path:

- `tvm_ffi/lib/libtvm_ffi.so`

Pre-built FlashInfer wheel sources:

- Stable CUDA 13.0 index root: `https://flashinfer.ai/whl/cu130/`
- Stable `flashinfer-jit-cache` wheel list: `https://flashinfer.ai/whl/cu130/flashinfer-jit-cache/`
- Nightly CUDA 13.0 index root: `https://flashinfer.ai/whl/nightly/cu130/`
- Nightly `flashinfer-jit-cache` wheel list: `https://flashinfer.ai/whl/nightly/cu130/flashinfer-jit-cache/`
- Installation guide: `https://docs.flashinfer.ai/installation.html`

## Why this v1 uses wheels instead of `flashinfer-cubin`
`gemma_rmsnorm` is already exposed from the host wrapper symbol `__tvm_ffi_gemma_rmsnorm` in `norm.so`. That wrapper is the stable user-facing entry for kernel launch and argument checking.

`flashinfer-cubin` is useful for workflows that separately download cubins and wire callback loaders. This v1 does not need that because `norm.so` already ships the compiled device code used by the wrapper.

## Cubin symbols vs API symbols
Cubin-level symbols are low-level, mangled kernel entry names generated by CUDA/C++. They are not intended as a stable external API.

The Rust integration calls the exported TVM-FFI host wrapper:

- `__tvm_ffi_gemma_rmsnorm`
- `__tvm_ffi_rmsnorm` (dispatches plain 2D RMSNorm vs fused QK 3D RMSNorm by input rank)
- `__tvm_ffi_gdn_prefill`
- `__tvm_ffi_append_paged_kv_cache` (from fixed `page.so`)
- `__tvm_ffi_append_paged_mla_kv_cache` (from fixed `page.so`)
- `__tvm_ffi_tgv_gemm` and `__tvm_ffi_tgv_gemm_tactic_num` (from `tgv_gemm_fp16.so` / `tgv_gemm_bf16.so`)
- `__tvm_ffi_trtllm_gemm_tactics` (from `trtllm_gemm.so`)
- `__tvm_ffi_trtllm_low_latency_gemm_tactics` and `__tvm_ffi_get_workspace_size_in_bytes` (from `trtllm_low_latency_gemm.so`)
- `__tvm_ffi_run` (for `single_prefill_with_kv_cache` JIT-cache modules)
- `__tvm_ffi_plan`, `__tvm_ffi_ragged_run`, and `__tvm_ffi_paged_run` (for `batch_prefill_with_kv_cache` JIT-cache modules)
- `__tvm_ffi_run` (for `single_decode_with_kv_cache` JIT-cache modules)
- `__tvm_ffi_plan` and `__tvm_ffi_run` (for `batch_decode_with_kv_cache` JIT-cache modules)

This wrapper handles argument decoding, validation, stream lookup, and dispatch to the correct kernel implementation.

## Dependency/artifact matrix
Pinned v1 artifacts:

- `flashinfer_jit_cache 0.6.3+cu130` pins for both `cu130` and `cu131` metadata keys (`x86_64` and `aarch64`)
- `apache_tvm_ffi 0.1.3` pins for both `cu130` and `cu131` metadata keys (`x86_64` and `aarch64`)

Runtime loading order:

1. Load `libtvm_ffi.so` with `RTLD_NOW | RTLD_GLOBAL`
2. Load `norm.so` with `RTLD_NOW | RTLD_LOCAL`
3. Load `gdn_prefill_sm90.so` with `RTLD_NOW | RTLD_LOCAL`
4. Load `page.so` with `RTLD_NOW | RTLD_LOCAL`
5. Load `single_prefill_with_kv_cache_*` modules on demand with `RTLD_NOW | RTLD_LOCAL`
6. Load `batch_prefill_with_kv_cache_*` modules on demand with `RTLD_NOW | RTLD_LOCAL`
7. Load `single_decode_with_kv_cache_*` modules on demand with `RTLD_NOW | RTLD_LOCAL`
8. Load `batch_decode_with_kv_cache_*` modules on demand with `RTLD_NOW | RTLD_LOCAL`
9. Load `tgv_gemm_fp16`/`tgv_gemm_bf16` modules on demand with `RTLD_NOW | RTLD_LOCAL`
10. Load `trtllm_gemm` module on demand with `RTLD_NOW | RTLD_LOCAL`
11. Load `trtllm_low_latency_gemm` module on demand with `RTLD_NOW | RTLD_LOCAL`

Required CUDA runtime dependency from `norm.so`:

- `libcudart.so.13`

`gdn_prefill_sm90` dispatch is `sm_90a`-only in the shipped kernel launcher. On non-SM90 GPUs the call will fail with a decoded TVM-FFI error.

## Why `StreamRestoreGuard` Is Needed
The TVM-FFI CUDA stream context is mutable runtime state. For each kernel call, Rust sets the active stream with:

- `TVMFFIEnvSetStream(kDLCUDA, device_id, stream, &old_stream)`

and receives the previous stream (`old_stream`) back.

`StreamRestoreGuard` is necessary so this temporary override is always reverted, including error paths:

1. It prevents stream-context leakage into later kernel calls.
2. It prevents accidental cross-library interference when other code also uses TVM-FFI stream state.
3. It preserves async correctness by restoring the prior context without adding a synchronization.

Implementation behavior:

1. Set stream and capture `old_stream`.
2. Create `StreamRestoreGuard`.
3. Launch wrapper (`__tvm_ffi_gemma_rmsnorm` / `__tvm_ffi_gdn_prefill` / `__tvm_ffi_append_paged_kv_cache` / `__tvm_ffi_run`).
4. Call `restore_now()` to surface stream-restore errors explicitly.
5. If control exits early, `Drop` performs a best-effort restore.

## Architecture-Dependent Kernel Handling
FlashInfer ships architecture-dependent device code and host wrappers that dispatch at runtime.

Packaging model:

1. `flashinfer-cubin`: pre-compiled kernel binaries covering supported architectures.
2. `flashinfer-jit-cache`: pre-built JIT cache bundles for specific CUDA versions.
3. `flashinfer-python`: Python package that can JIT/download kernels on first use.

Runtime behavior in this Rust integration:

1. Rust calls exported host wrappers such as `__tvm_ffi_gemma_rmsnorm`, `__tvm_ffi_gdn_prefill`, and `__tvm_ffi_append_paged_kv_cache`.
2. The TVM host wrapper dispatches by kernel variant ABI (dtype, head dims, mask/layout options, etc.) and validates tensor contracts.
3. GPU architecture selection is usually handled by CUDA loading the matching cubin from the module fatbin (`sm_90a`, `sm_100a`, `sm_120`, etc.).
4. If the loaded artifact does not contain compatible device code for the active GPU, launch fails and the error is surfaced as `FlashInferError::TvmFfiCall`.

What dispatches what:

1. Host wrapper dispatch (TVM-FFI): exported `run`/named functions, argument decoding, validation, and variant selection.
2. Architecture dispatch (CUDA): pick the best embedded cubin for the current device at module load/launch time.
3. Explicit-arch launchers (exception): some kernels perform manual compute-capability checks and reject unsupported devices directly in host code.

For MHA single prefill, Rust constructs the exact wheel URI from runtime params:

- `single_prefill_with_kv_cache_dtype_q_{...}_dtype_kv_{...}_dtype_o_{...}_head_dim_qk_{...}_head_dim_vo_{...}_posenc_{...}_use_swa_{...}_use_logits_cap_{...}_f16qk_{...}`

That module is extracted lazily from the wheel and cached per-process by URI.

For paged KV append kernels, Rust resolves fixed symbols from the fixed `page.so` artifact at runtime initialization:

- `__tvm_ffi_append_paged_kv_cache`
- `__tvm_ffi_append_paged_mla_kv_cache`

For GEMM in this integration:

- `tgv_gemm` is bound for MM-only semantics (`A [m,k]`, `B [k,n]`, `out [m,n]`) with `f16`/`bf16`.
- Rust maps MM semantics to the upstream ABI by passing transposed views (`mat1=B^T`, `mat2=A^T`) without copies.
- f16/bf16 BMM is intentionally out of scope in this change because corresponding wheel exports are not present in the pinned artifacts.

For TRTLLM GEMM support:

- `trtllm_gemm_tactics` is exposed from `trtllm_gemm.so`.
- `trtllm_low_latency_gemm_tactics` and `get_workspace_size_in_bytes` are exposed from `trtllm_low_latency_gemm.so`.
- Array results (`Array<int64_t>`) are decoded via TVM globals `ffi.ArraySize` and `ffi.ArrayGetItem`.

For MHA batched ragged prefill (FA2 path), Rust similarly constructs:

- `batch_prefill_with_kv_cache_dtype_q_{...}_dtype_kv_{...}_dtype_o_{...}_dtype_idx_i32_head_dim_qk_{...}_head_dim_vo_{...}_posenc_{...}_use_swa_{...}_use_logits_cap_{...}_f16qk_{...}`

It calls `plan` first, converts returned `AnyView` into an owned TVM object, passes that plan object to `ragged_run` or `paged_run`, and decref's it after launch.

For MHA single decode, Rust constructs:

- `single_decode_with_kv_cache_dtype_q_{...}_dtype_kv_{...}_dtype_o_{...}_head_dim_qk_{...}_head_dim_vo_{...}_posenc_{...}_use_swa_{...}_use_logits_cap_{...}`

For MHA batched paged decode, Rust constructs:

- `batch_decode_with_kv_cache_dtype_q_{...}_dtype_kv_{...}_dtype_o_{...}_dtype_idx_i32_head_dim_qk_{...}_head_dim_vo_{...}_posenc_{...}_use_swa_{...}_use_logits_cap_{...}`

Batched decode also uses a two-step host ABI:

1. `plan` (`__tvm_ffi_plan`) to produce opaque plan info.
2. `run` (`__tvm_ffi_run`) with that plan object and runtime tensors.

Concrete example:

- `gdn_prefill` checks device major capability and only dispatches the SM90A implementation in the current launcher (`flashinfer/csrc/gdn_prefill_launcher.cu`).
- `single_prefill_with_kv_cache` exports a generic `run` symbol and dispatches variant parameters in host code (`flashinfer/csrc/single_prefill_jit_binding.cu`, `flashinfer/csrc/single_prefill.cu`), while architecture is selected from embedded cubins by CUDA.

Build-from-source note:

- FlashInfer uses `FLASHINFER_CUDA_ARCH_LIST` to control which architectures are compiled into generated artifacts.

## CUTLASS Fused MoE Profile IDs
For CUTLASS fused MoE, the host `run_moe` entry accepts optional profile IDs (`profile_ids`) that select GEMM tactics:

- `profile_ids[0]`: GEMM1 profile id
- `profile_ids[1]`: GEMM2 profile id

In `flashinfer-rs`, this is exposed as:

- `FusedMoeParams::with_profile_ids(gemm1_profile_id, gemm2_profile_id)`
- `FusedMoeCudarcOptions { profile_ids: Some([gemm1, gemm2]), .. }`

If `profile_ids` is omitted, Rust passes `None` and FlashInfer host code uses its default tactic selection path.

## Runtime configuration knobs
Environment variables accepted by `flashinfer-rs`:

- `FLASHINFER_RS_CACHE_DIR`

Runtime wheel cache:

- `~/.cache/flashinfer-rs/wheels/<sha256>-<filename>.whl`

Extracted shared-library cache:

- `~/.cache/flashinfer-rs/<artifact-hash>/`

Both wheel materialization and `.so` extraction use lock files to avoid concurrent races.
